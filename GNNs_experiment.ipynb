{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GNNs experiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### prepare environment\n",
        "We use PyTorch Geometric (PyG) to conduct our experiment. PyG provides a set of graph benchmark dataset and standard graph neural networks (GNNs) model.\n",
        "\n",
        "**Note**: the code used in this experiment is following [PyG tutorial](https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html)"
      ],
      "metadata": {
        "id": "wx9wPbTM6xky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oepOaqM66nvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ade7475-850d-4c13-885e-5b1488ae6a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 43.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 22.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 33.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 63.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.7 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install required packages for PyG\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import all necessary libraries\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric import datasets\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "torch.manual_seed(2022)  # to reproduce the result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NzJb0rx6rRV",
        "outputId": "dbc2055c-5474-437f-f4e2-1665d4e0ba41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efba1d700f0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "We use two different datasets to compare how a GNNs model perform on them.\n",
        "\n",
        "The first dataset is a standard citation network dataset, `Cora`. In this dataset, nodes represent scientific documents and edges represent the citation links between them. The dataset is well-cleaned with ~ 2.7k nodes. Each node contains rich features from its text content. We use the default training and testing set that were splitted by [paper](https://arxiv.org/abs/1603.08861)\n",
        "\n",
        "The second dataset is a collection from Amazon products, [source](https://arxiv.org/abs/1811.05868). Nodes represent products and edges represent a fact that 2 products are in the same order (bought together). We use the large dataset, `Computers`, with ~ 13k nodes. Node features are users' review represented as bag-of-word vector (_claim_: not a high quality representation). This dataset is larger and also noiser than the first one, it's inherently a challenge for our model.\n",
        "\n",
        "Let's download the datasets and see some statistics about them!"
      ],
      "metadata": {
        "id": "Zz_86M-q6sRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataset\n",
        "cora_ds = datasets.Planetoid(root='data/Planetoid', name='Cora')\n",
        "amazon_ds = datasets.Amazon(root='data/Amazon', name='Computers')\n",
        "\n",
        "# get some stats\n",
        "ds_names = ['Cora', 'Amazon']\n",
        "data_stats = {\n",
        "    'Cora': {},\n",
        "    'Amazon': {}\n",
        "}\n",
        "\n",
        "for name, dataset in zip(ds_names, [cora_ds, amazon_ds]):\n",
        "  data_stats[name]['Number of graphs'] = len(dataset)\n",
        "  data_stats[name]['Number of features'] = dataset.num_features\n",
        "  data_stats[name]['Number of classes'] = dataset.num_classes\n",
        "  data_stats[name]['Number of nodes'] = dataset[0].num_nodes\n",
        "  data_stats[name]['Number of edges'] = dataset[0].num_edges\n",
        "  data_stats[name]['Number of node per class'] = dict(Counter(dataset[0].y.tolist()))\n",
        "  data_stats[name]['Average node degree'] = round(dataset[0].num_edges / dataset[0].num_nodes)\n",
        "  data_stats[name]['Train-test prepared?'] = hasattr(dataset[0], \"train_mask\")\n",
        "  data_stats[name]['Has isolated nodes'] = dataset[0].has_isolated_nodes()\n",
        "  data_stats[name]['Has self-loops'] = dataset[0].has_self_loops()\n",
        "  data_stats[name]['Is undirected graph'] = dataset[0].is_undirected()\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = ['Attribute'] + ds_names\n",
        "for attribute in sorted(data_stats[ds_names[0]].keys(), reverse=True):\n",
        "  table.add_row([attribute, data_stats[ds_names[0]][attribute], data_stats[ds_names[1]][attribute]])\n",
        "\n",
        "print('Dataset stats comparison:')\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5C71Vsk-2mm",
        "outputId": "e17dd721-8250-4fe3-f1cb-86fbba50ab47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_computers.npz\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset stats comparison:\n",
            "+--------------------------+----------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
            "|        Attribute         |                           Cora                           |                                        Amazon                                        |\n",
            "+--------------------------+----------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
            "|   Train-test prepared?   |                           True                           |                                        False                                         |\n",
            "|     Number of nodes      |                           2708                           |                                        13752                                         |\n",
            "| Number of node per class | {3: 818, 4: 426, 0: 351, 2: 418, 1: 217, 5: 298, 6: 180} | {4: 5158, 8: 2156, 6: 487, 1: 2142, 3: 542, 9: 291, 2: 1414, 0: 436, 7: 818, 5: 308} |\n",
            "|     Number of graphs     |                            1                             |                                          1                                           |\n",
            "|    Number of features    |                           1433                           |                                         767                                          |\n",
            "|     Number of edges      |                          10556                           |                                        491722                                        |\n",
            "|    Number of classes     |                            7                             |                                          10                                          |\n",
            "|   Is undirected graph    |                           True                           |                                         True                                         |\n",
            "|      Has self-loops      |                          False                           |                                        False                                         |\n",
            "|    Has isolated nodes    |                          False                           |                                         True                                         |\n",
            "|   Average node degree    |                            4                             |                                          36                                          |\n",
            "+--------------------------+----------------------------------------------------------+--------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intuitively, the Amazon dataset will be the harder for our model:\n",
        "  - it has more nodes, larger `average node degree` means our computation graph will be expanded.\n",
        "  - some nodes are isolated and negatively contribute to the model.\n",
        "  - unbalance: class `4` has 5k samples, while class `9` and `5` have only ~ 300 samples.\n",
        "  - ...\n",
        "\n",
        "Thanks to the Cora dataset, it's quite clean and we do not have to do anything with it except normalizing features. But the Amazon dataset is unbalance between classes and we need to pick our training/testing set. From the statistics table, we will keep number of training at 100 nodes/class when splitting."
      ],
      "metadata": {
        "id": "qUVDUn77DfcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define transformation operator for amazon dataset\n",
        "# randomly choose train/test sample\n",
        "transform_op = T.Compose([\n",
        "  T.NormalizeFeatures(),\n",
        "  T.RandomNodeSplit(split='random', num_train_per_class=100, num_test=0.2)\n",
        "])\n",
        "\n",
        "# get the first graph from dataset and apply transformed operator\n",
        "amazon_graph = transform_op(amazon_ds[0])\n",
        "print('Amazon graph: ', amazon_graph)\n",
        "\n",
        "# normalize cora graph\n",
        "cora_graph = T.NormalizeFeatures()(cora_ds[0])\n",
        "print('Cora graph: ', cora_graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHS-QmUDGXRd",
        "outputId": "6416080d-717d-4db7-bf5d-34327494747b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon graph:  Data(x=[13752, 767], edge_index=[2, 491722], y=[13752], train_mask=[13752], val_mask=[13752], test_mask=[13752])\n",
            "Cora graph:  Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our training dataset again"
      ],
      "metadata": {
        "id": "q0S8t9c0ISSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table = PrettyTable()\n",
        "table.field_names = ['Attribute', 'Cora', 'Amazon']\n",
        "table.add_row(['training', cora_graph.train_mask.sum().item(), amazon_graph.train_mask.sum().item()])\n",
        "table.add_row(['testing', cora_graph.test_mask.sum().item(), amazon_graph.test_mask.sum().item()])\n",
        "table.add_row(['validation', cora_graph.val_mask.sum().item(), amazon_graph.val_mask.sum().item()])\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb9sbjRjHSKK",
        "outputId": "fd79196b-7113-4162-8fed-ddc39a06979c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------+--------+\n",
            "| Attribute  | Cora | Amazon |\n",
            "+------------+------+--------+\n",
            "|  training  | 140  |  1000  |\n",
            "|  testing   | 1000 |  2750  |\n",
            "| validation | 500  |  500   |\n",
            "+------------+------+--------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "Now we define a simple model with 2 layers GCN layer, each layer is equivalent to a neural-message-passing iteration."
      ],
      "metadata": {
        "id": "o06lnfQiIWq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels, num_classes):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(11)\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_idx):\n",
        "        # 1st neural message passing\n",
        "        x = self.conv1(x, edge_idx)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        # 2nd neural message passing\n",
        "        x = self.conv2(x, edge_idx)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ROY_ZQAgIkrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create 2 models for 2 datasets with the same architecture\n",
        "cora_model = GCN(num_features=cora_ds.num_features, hidden_channels=16, num_classes=cora_ds.num_classes)\n",
        "amazon_model = GCN(num_features=amazon_ds.num_features, hidden_channels=16, num_classes=amazon_ds.num_classes)\n",
        "\n",
        "print('GCN cora:', cora_model)\n",
        "print()\n",
        "print('GCN amazon:', amazon_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYYT--V_I_Vt",
        "outputId": "c9ced3bd-022f-4fc0-b92c-b0571358f8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN cora: GCN(\n",
            "  (conv1): GCNConv(1433, 16)\n",
            "  (conv2): GCNConv(16, 7)\n",
            ")\n",
            "\n",
            "GCN amazon: GCN(\n",
            "  (conv1): GCNConv(767, 16)\n",
            "  (conv2): GCNConv(16, 10)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "DZC6tcfzI5sE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we train the `GCN Cora` model since the Cora dataset is small so it takes less computation and time to learn."
      ],
      "metadata": {
        "id": "eQyYTPDRI33w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(cora_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train_cora():\n",
        "      cora_model.train()\n",
        "      optimizer.zero_grad()  # clear gradients from previous iteration\n",
        "\n",
        "      out = cora_model(cora_graph.x, cora_graph.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[cora_graph.train_mask], cora_graph.y[cora_graph.train_mask])  # here we use only training nodes for backward step\n",
        "      loss.backward()  # calculate gradient\n",
        "      optimizer.step()  # update model's parameters\n",
        "      return loss\n",
        "\n",
        "def test_cora():\n",
        "      cora_model.eval()\n",
        "      out = cora_model(cora_graph.x, cora_graph.edge_index)\n",
        "      pred = out.argmax(dim=1)  # get the class with highest probability as prediction\n",
        "      test_correct = (pred[cora_graph.test_mask] == cora_graph.y[cora_graph.test_mask])  # check model agains true label in test set\n",
        "      test_acc = int(test_correct.sum()) / int(cora_graph.test_mask.sum())  # get ratio of correct predictions as accuracy\n",
        "      return test_acc*100\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(0, 100):\n",
        "    loss = train_cora()\n",
        "    if epoch % 10 == 0:\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "print('---')\n",
        "test_acc = test_cora()\n",
        "print(f'Test Accuracy for Cora model: {test_acc:.4f}')\n",
        "\n",
        "print(f'time executed: {time.time() - start_time:.2f} secs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK5OSDGgJAvY",
        "outputId": "de6d1e21-b521-482b-8fe2-3f27b069d20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 1.9467\n",
            "Epoch: 010, Loss: 1.8673\n",
            "Epoch: 020, Loss: 1.7243\n",
            "Epoch: 030, Loss: 1.5250\n",
            "Epoch: 040, Loss: 1.3139\n",
            "Epoch: 050, Loss: 1.1252\n",
            "Epoch: 060, Loss: 0.9324\n",
            "Epoch: 070, Loss: 0.8099\n",
            "Epoch: 080, Loss: 0.6694\n",
            "Epoch: 090, Loss: 0.5821\n",
            "---\n",
            "Test Accuracy for Cora model: 82.2000\n",
            "time executed: 2.02 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's interesting that the training size is quite small, only 140 samples are provided, but the model was able to learn Cora representation and made a good prediction about 1000 other nodes, with accuracy of 82%.\n",
        "\n",
        "Let's train the GCN Amazon, notice that we set all parameters the same but definitely with more iterations!"
      ],
      "metadata": {
        "id": "R0lBTY2jLu18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(amazon_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train_amazon():\n",
        "      amazon_model.train()\n",
        "      optimizer.zero_grad()  # clear gradients from previous iteration\n",
        "\n",
        "      out = amazon_model(amazon_graph.x, amazon_graph.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[amazon_graph.train_mask], amazon_graph.y[amazon_graph.train_mask])  # here we use only training nodes for backward step\n",
        "      loss.backward()  # calculate gradient\n",
        "      optimizer.step()  # update model's parameters\n",
        "      return loss\n",
        "\n",
        "def test_amazon():\n",
        "      amazon_model.eval()\n",
        "      out = amazon_model(amazon_graph.x, amazon_graph.edge_index)\n",
        "      pred = out.argmax(dim=1)  # get the class with highest probability as prediction\n",
        "      test_correct = (pred[amazon_graph.test_mask] == amazon_graph.y[amazon_graph.test_mask])  # check model agains true label in test set\n",
        "      test_acc = int(test_correct.sum()) / int(amazon_graph.test_mask.sum())  # get ratio of correct predictions as accuracy\n",
        "      return test_acc*100\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(0, 800):\n",
        "    loss = train_amazon()\n",
        "    if epoch % 50 == 0:\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "print('---')\n",
        "test_acc = test_amazon()\n",
        "print(f'Test Accuracy for Amazon model: {test_acc:.4f}')\n",
        "\n",
        "print(f'time executed: {(time.time() - start_time)/60:.2f} mins')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RRFYduEMGxy",
        "outputId": "2edc2fcb-631b-451e-bc20-857c5966ce35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 2.3024\n",
            "Epoch: 050, Loss: 2.2193\n",
            "Epoch: 100, Loss: 2.0553\n",
            "Epoch: 150, Loss: 1.8726\n",
            "Epoch: 200, Loss: 1.7015\n",
            "Epoch: 250, Loss: 1.5806\n",
            "Epoch: 300, Loss: 1.5130\n",
            "Epoch: 350, Loss: 1.4458\n",
            "Epoch: 400, Loss: 1.4036\n",
            "Epoch: 450, Loss: 1.3624\n",
            "Epoch: 500, Loss: 1.3403\n",
            "Epoch: 550, Loss: 1.3395\n",
            "Epoch: 600, Loss: 1.3072\n",
            "Epoch: 650, Loss: 1.3045\n",
            "Epoch: 700, Loss: 1.2805\n",
            "Epoch: 750, Loss: 1.2840\n",
            "---\n",
            "Test Accuracy for Amazon model: 69.8182\n",
            "time executed: 5.82 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GCN Amazon model is promising to converge with 70% accuracy. This toy experiment presents the powerful of GNNs model with the ability to generalize in different datasets.\n",
        "\n",
        "However, with a larger graph dataset, more nodes and more relations, this experiment shows the computation challenge of neural-message-passing. Imagine a social networks with billion of nodes, how many hours it will take to learn a good representation!"
      ],
      "metadata": {
        "id": "f2nJxMbVPsdL"
      }
    }
  ]
}